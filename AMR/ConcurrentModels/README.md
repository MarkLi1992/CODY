# Concurrent Models for Adaptive Mesh Refinement Using a Pointer based Quad Tree Data Structure
---

## Description
---

We have two implementations of a concurrent task based model on adaptive mesh 
refinement codes using two different languages: Go and D. We wanted to test these 
new languages with built in concurrency primitives on code models used for 
scientific computation that had previously been parallelized in the legacy style 
of bulk synchrony and static schedulers.  We developed a very simple
benchmarking test where each leaf node of the tree executes a certain amount of
work in the form of an empty for-loop.  The general process involves traversing
the tree, adding each leaf node to the work queue with the corresponding work
function, and then executing on the work in the queue.  We time the process
using different numbers of cores to do the work in order to examine speed up.
We have a serial method to compare for speed up and overhead costs. 

## Installation
---
* Download and install Go and D:
	* http://golang.org/doc/install
	* http://dlang.org/download.html
* To run the post processing data analysis you need Python and Numpy
* Pull the repository
* To run the programs there are three .sh files 
	* tests.sh
		* Runs Go and D code on a increaseing large amounts dummy work
		  ranging from 0.25 milliseconds to one millisecond on my
		  computer.  See below for details on how to determine the
		  runtime of the dummy work.
	* go.sh 
		* Command line arguments for data output file, maximum depth of
		  the tree, the number of iterations to test the work queue, the
		  number of dummy iterations, ability to set maximum cores
		* Compile using: `go build -o QuadTree`	
	* d.sh	
		* Command line arguments for data output file, maximum depth of
		  the tree, the number of iterations to test the work queue, the
		  number of dummy iterations, ability to set maximum cores
		* Compile using: `dmd QuadTree.d`
* Directories
	* dTree and QuadTree.d
		* Tree has initial minimum depth of 4, can be changed in the
		  code
	* goTree and QuadTree.go
		* Tree has initial minimum depth of 4, can be changed in the
		  code
* Timing
	* For each code the first command line argument specifies which function
	  to run: 0 runs concurrent tests, 1 runs serial tests, and 2 runs
	  timing tests.  In order to figure out how many dummy iterations to use 
	  for testing first use case 2 to determine the timing of the dummy work.
	* For the command line arguments, see the comments in the go.sh and d.sh
	  files
	*  

## Go
---
* Open source systems language designed by Google
* Garbage Collected
* Built in Concurrency Primitives:
	* Goroutine: lightweight execution thread that is mapped to hardware
	  threads
	* Goroutines communicate via channels using the paradigm: do not
	  communicate by sharing memory, instead share memory by communicating.	
* Worker Queue Model: (None built-in)
	* Initiate channel to act as work queue
	* Spawn as many goroutines as hardware threads
	* Traverse the quad tree and add dummy leaf work to the queue
	* Once all the work has been added pass nil values to queue to indicate
	  all work has been completed


## D
---
* Open source systems language
* Very similar to C/C++ with some other influences (i.e. it has garbage
  collection)
* Built-in worker queue model based on std.concurrency library.
	* Traverse the quad tree and add tasks to the TaskPool which consist of
	  a node and the dummy function
	* The TaskPool handles the execution of the tasks

## Post-Processing Results
---
I use python scripts to anaylze the data generated by the Go and D code. 
* postProcess.py
	* Command line arguments: 
		* Directory names (number must be set in code, currently 2)
		* Maximum number of cores
		* Output file names (currently 4 * number of directories, 4 here
		  is the number of dummy work time tests)
	* It generates figures for speedup, strong scaling, and various
	  timing metrics
	* The dictionaries are saved as pickle files for reuse
* overhead.py
	* Graphs the overhead cost of concurrency using the formula:
		* ((0 Concurrent Work) - (0 Serial Work))/(Total Concurrent
		  Work) 
	* Command line arguments: input files (data dictionary saved as pickle
	  file) and output file names, extensions are unnessary 
	* Max Cores and the number of data files need to be hard coded


## Testing Results
---
